---
title: "A classification analysis of customer churn"
subtitle: "Applied machine learning and big data, fall 2021"
author: "Mikkel Wittenburg Petersen"
date: "`r Sys.Date()`"
toc-title: Contents
output:
  pagedown::html_paged:
    toc: true
    fig_caption: true
    number_sections: true
    css:
      - "../styles/fonts.css"
      - default
      - "../styles/custom-style.css"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(reticulate)
library(gt)
library(gtsummary)
library(tidyverse)
```

```{python}
import pandas as pd
import numpy as np
import random
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.metrics import roc_curve, auc, roc_auc_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn import svm
from sklearn.model_selection import GridSearchCV


```

```{python, include=FALSE}
# Import the data
df_churn = pd.read_csv("../data/telecom-churn.csv")

```

# Introduction
In this section I explore how well different classification algorithms are able to predict customer churn in a data set with customer level information from a telecommunications company. For such a company being able to retain customers is of course important. From an analytics perspective, one way of improving retention is being able to predict which customers are likely to stop using the company's services.

To predict this I include 3 different classification algorithms in the analysis: logistic regression, k-nearest neighbors (kNN) and Support Vector Machines (SVM. All 3 are well suited for when the outcome variable is binary, as is the case in this analysis (churn/no churn). 

The data set is from Kaggle and can be found here: https://www.kaggle.com/barun2104/telecom-churn. It contains 11 columns including the outcome column, and 3333 rows. Each row contains information about a customer.

# Method
In this section I describe how I will go about predicting customer churn from the telecom data set. Specifically, I describe the data, the process of supervised machine learning, the algorithms used in this analysis, and how the performance of algorithms can be measured.

## The data set
The data set contains the following 11 columns:

:::: {style="display: flex;"}

::: {}
- `r colnames(py$df_churn)[1]`: has the customer canceled the service or not.
- `r colnames(py$df_churn)[2]`: number of weeks the customer has had an active account.
- `r colnames(py$df_churn)[3]`: has the customer recently renewed the service contract or not.
- `r colnames(py$df_churn)[4]`: does the customer have a data plan or not.
:::

::: {}
- `r colnames(py$df_churn)[5]`: gigabytes of monthly data usage.
- `r colnames(py$df_churn)[6]`: number of calls into customer service.
- `r colnames(py$df_churn)[7]`: average daytime minutes per day.
- `r colnames(py$df_churn)[8]`: average number of daytime calls.
:::

::: {}
- `r colnames(py$df_churn)[9]`: average monthly bill.
- `r colnames(py$df_churn)[10]`: largest overage fee in last 12 months.
- `r colnames(py$df_churn)[11]`: average number of roaming minutes.
:::

::::

The first row `r colnames(py$df_churn)[1]` is the outcome variable of interest. It has the value 1 if the customer canceled the service, 0 if not. 14.5 % of the 3333 customers in the data set have canceled their service. This means that there's class imbalance in the outcome variable. The rest of the variables in the data set are the predictor variables. 

Here’s a look at the first 3 rows and columns of the data set:
```{r}

gt(head(py$df_churn[1:3], 3)) %>%
  tab_options(table.align='left',
              table.width = pct(100),
              column_labels.border.bottom.width= px(3),
              column_labels.border.bottom.color= "black",
              column_labels.border.top.color = "white") %>%
  tab_style(
    style = list(
      cell_text(weight = "bold")
      ),
    locations = cells_column_labels()) %>%
  tab_style(
    style = list(
      cell_fill(color = "white")
    ),
    locations = cells_body(
      rows = 2)
  ) 

```

## The process of supervised machine learning 
The task of predicting a binary outcome such as churn falls under supervised machine learning. In supervised learning the goal is to build a statistical model to predict an outcome based on one or more predictor variables (James et al. 2021:1). When the outcome is categorical (as churn) it is described as classification (as opposed to regression when the outcome variable is quantitative).

In supervised machine learning the starting point is a data set that includes both the predictor variables and the outcome variable. A key idea is to then divide the data set into a training set and a test set. The training set is used to fit the statistical model to the data. Afterwards the test set is used to evaluate how good the model is at predicting the outcome variable of interest (Kabacoff 2021:449). This is done by using the predictor variables in the test set and the trained statistical model to predict the class of each observation in the test set. These predictions are then compared to the actual values of the outcome variable in the test set. This evaluation is used to assess how good the model will perform in future situations where we only have access to a set of predictor variables and not the actual outcome variable (Irizarry 2019:503).

## Classification algorithms used in this analysis
In this section I give a very brief description of logistic regression, k-nearest neighbors and Support Vector Machines.

### Logistic regression
Logistic regression is a variant of the general linear model. It is similar to linear regression in several ways. The right hand side of the equations are the same, and the decision rule produced by both is a line (Irizarry 2019:567). But in logistic regression we don't predict the outcome y, we predict the log odds of y (James et al. 2021:133). Thus the output is conditional probabilities of the outcome variable given the predictor variable(s), expressed in log odds units. In classification these probabilities are used to predict class membership. Another difference between logistic regression and linear regression is that logistic regression uses the maximum likelihood estimate, and not the least squares estimate.

### k-nearest neighbors (kNN)
Like logistic regression, the kNN algorithm also estimates the conditional probabilities of the outcome variable given the predictors, and then uses these probabilities to predict the class membership (James et al. 2021:39). It does so by estimating local probabilities. So in order to predict the class of a data point, the model looks at the 'k' nearest data points. And the class that is the most frequent among the k closest data points become the predicted class. The k nearest points are defined by the distance between them based on the features (Irizarry 2019:536). kNN, as a non-parametric method, provide a much more flexible alternative to for instance logistic regression (James et al. 2021:105). We can control the flexibility with the k hyperparameter. Larger k's result in more inflexible estimates.

### Support Vector Machines (SVM)
SVM's are based on the concept of so-called separating hyperplanes (James et al. 2021:369). The algorithm uses these hyperplanes to determine decision rules. The role of a hyperplane is to maximize the margin between the closest points of two classes, and thus separate the data points of these classes (Kabacoff 2021:460). Data points closest to the hyperplane define the boundary of the margins of the hyperplane. These are called support vectors. The most simple separator is a line (in a 2 dimensional space), but it can also take the form of a N-dimensional hyperplane, depending on the data (Kabacoff 2021:460). When there is a non-linear relationship between data of interest SVM uses so-called kernels to "tranform the data into higher dimensions" (Kabacoff 2021:460).

## Resampling and tuning of hyperparameters
Parameters in machine learning algorithms that need to be specified before fitting a model are called hyperparameters. These are parameters that optimize how well the model fit the data. k in kNN is an example of a hyperparameter. The method for choosing such parameters is inherently explorative: you fit the models separately with different hyperparameter values and choose the one that performs the best. I have done this with each of the 3 models in this analysis, using k-fold cross validation on the training set.

## Measuring model performance
There are several metrics for measuring model performance when predicting a categorical outcome. Below I briefly describe the metrics I will use in this analysis to assess the performance of the models I have included:

- **Overall accuracy**: the fraction of cases that were correctly predicted in the test set (Kabacoff 2021:465). Calculated as the number of correct predictions divided by the total number of predictions.

- **Sensitivity (or recall or true positive rate)**: The proportion of actual positives that are predicted as positives (Irizarry 2019:508). In this analysis it's the percentage of people who have stopped using the service that are correctly identified as such in the test set. In other words – In the case of customer churn – high sensitivity means that the model has predicted most of the actual churn correctly. Calculated as the number of true positives divided by the number of true positives + the number of false negatives.

- **Specificity (or true negative rate)**: The proportion of actual negatives that are predicted as negatives (Irizarry 2019:508). In this analysis it's the percentage of people who are still using the service that are correctly identified as such. Calculated as the number of true negatives divided by the number of true negatives + the number of false positives.

- **Precision (or positive predictive value)**: The proportion of predicted positives that are actual positives (Irizarry 2019:508). So if a person is classified as having stopped using the service how likely is it that this classification will be correct? In other words – in the case of customer churn – high precision means that the algorithm rarely predicts churn when the customer has in fact stayed. Calculated as the number of true positives divided by the number of true positives + the number of false positives.

For each of the 3 models I include in this analysis I will present these metrics through confusion matrices and classification reports.

# Analysis

```{python, include=FALSE}
# #split dataset in features and target variable
random.seed(10)

x = df_churn.drop('Churn', axis=1)

y = df_churn['Churn']

```


```{python, message=FALSE}
# split X and y into training and testing sets

x_train,x_test,y_train,y_test = train_test_split(x, y, test_size = 0.25, random_state = 0, stratify = y)

```


```{python, include=FALSE, message=FALSE}
# Standardize the predictor variables that aren't binary

preproc = ColumnTransformer(
    transformers=[
        ('scale', StandardScaler(), 
        ["AccountWeeks", "DataUsage", "CustServCalls", "DayMins", "DayCalls", "MonthlyCharge", "OverageFee", "RoamMins"]),
    ],
    remainder="passthrough",
)

x_train = preproc.fit_transform(x_train)

x_test = preproc.fit_transform(x_test)

```

## Logistic regression

```{python, include=FALSE, warning=FALSE}

# Create the hyperparameter grid
c_space = [0.01, 0.1, 1, 10]

param_grid = {'C': c_space, 'penalty': ['l1', 'l2'], 'solver' : ['newton-cg', 'lbfgs', 'liblinear']}

# instantiate the model
logreg_model = LogisticRegression()

# Instantiate the GridSearchCV object: logreg_cv
logreg_cv = GridSearchCV(logreg_model, param_grid, cv=5)

# fit the model with data
logreg_cv.fit(x_train,y_train)

# Predict churn in the test set
logreg_prediction_y = logreg_cv.predict(x_test)

```

```{python, include=FALSE}

# Create dataframe with actual and predicted values of y
logreg_prediction_y_series = pd.Series(logreg_prediction_y, name = 'predicted_y')

y_test_actual = y_test.reset_index(drop = True).rename('actual_y')

df_logreg_results = pd.concat([y_test_actual, logreg_prediction_y_series],axis=1)



```

```{r}

# Create dataframe with results for the confusion matrix
df_logreg_results <- py$df_logreg_results %>%
  mutate(actual_y_text = case_when(
    actual_y == 1 ~ "Churn",
    actual_y == 0 ~ "No churn"),
    predicted_y_text = case_when(
      predicted_y == 1 ~ "Churn",
      predicted_y == 0 ~ "No churn"
    ))

```

```{r, include=FALSE}

# Create confusion matrix
logreg_confusion_matrix <- tbl_cross(
  df_logreg_results,
  row = actual_y_text,
  col = predicted_y_text,
  margin=NULL,
  list(actual_y_text ~ "Actual label", predicted_y_text ~ "Predicted label")
) %>%
  modify_header(
    update = list(
      label ~ "")
  ) %>%
  modify_caption("**Confusion matrix of the logistic regression prediction**") %>%
  as_gt() %>%
  tab_style(
    style = list(
      cell_fill(color = "white")
    ),
    locations = cells_body(
      rows = everything())
  ) %>%
  tab_options(table.align='left',
              table.width = pct(100),
              column_labels.border.top.color = "white")

```

```{python, include=FALSE}

# Accuracy score
logreg_accuracy_score = round(metrics.accuracy_score(y_test, logreg_prediction_y), 2)

# Precision and recall scores for churn = 0
logreg_precision_score_0 = round(metrics.precision_score(y_test, logreg_prediction_y, pos_label = 0), 2)

logreg_recall_score_0 = round(metrics.recall_score(y_test, logreg_prediction_y, pos_label = 0), 2)

# Precision and recall scores for churn = 1
logreg_precision_score_1 = round(metrics.precision_score(y_test, logreg_prediction_y, pos_label = 1), 2)

logreg_recall_score_1 = round(metrics.recall_score(y_test, logreg_prediction_y, pos_label = 1), 2)
```

The first model I have used to predict churn is a logistic regression model. We can see the results of the prediction on the test set in the confusion matrix in table \@ref(tab:logreg-confusion) below. From the confusion matrix we can compute the overall accuracy, which is `r py$logreg_accuracy_score`. This is not so bad. A quick glance at the table reveals an imbalance though: it seems that the model is pretty good at predicting "no churn", but not so good at predicting churn (which is the thing that we are most interested in being able to predict).   

```{r logreg-confusion}

logreg_confusion_matrix  

```

```{python, include=FALSE, message=FALSE}

# Classification report where the output is a dictionary
report_logreg_prediction = classification_report(y_test, logreg_prediction_y, output_dict = True)

# Convert the dict to a dataframe
df_report_logreg_prediction = pd.DataFrame(report_logreg_prediction).transpose().reset_index().rename(columns={'index': 'bar'})

df_report_logreg_prediction[['precision','recall', 'f1-score', 'support']] = df_report_logreg_prediction[['precision','recall', 'f1-score', 'support']].apply(lambda x: pd.Series.round(x, 2))
```

```{r}

# Convert from pandas to R dataframe
df_report_logreg_prediction <- py$df_report_logreg_prediction %>%
  slice(c(1, 2, 5))

```

The accompanying classification report with precision, recall and specificity scores is presented in table \@ref(tab:logreg-classification-report). Here we see that the precision and recall scores for 0 (no churn) are good (`r py$logreg_precision_score_0` and `r py$logreg_recall_score_0` respectively). The recall score of the negative class (in this case "no churn") in a binary classification is the specificity score. These numbers support the conclusion from the quick glance above. From the table we can see that the precision score for churn = 1 is `r py$logreg_precision_score_1`. The score tells us the proportion of predicted churn that are actual churn. The recall score is `r py$logreg_recall_score_1`. The score is the proportion of actual churn that are predicted as churn. This metric may be the most important metric for a telecom company who wants to be able to predict churn. The very low proportion is not good news. 

```{r logreg-classification-report}

# Publication ready classification report
gt(df_report_logreg_prediction,
  caption = md("**Classification report of the logistic regression prediction**")) %>%
  tab_options(table.align='left',
              table.width = pct(100),
              column_labels.border.bottom.width= px(3),
              column_labels.border.bottom.color= "black",
              column_labels.border.top.color = "white") %>%
  tab_style(
    style = list(
      cell_text(weight = "bold")
      ),
    locations = cells_column_labels()) %>%
  tab_style(
    style = list(
      cell_fill(color = "white")
    ),
    locations = cells_body(
      rows = everything())
  ) %>% 
  tab_style(
    style = list(
       cell_text(weight = "bold")
    ),
    locations = cells_body(
      columns = 1)
  ) %>%
  cols_label(
    bar = ""
  )

```

The logistic regression model didn't do a very impressive job at predicting churn. This may be the result of an imbalanced outcome variable. Another explanation could be the model's inability to capture non-linear relationships with its linear decision boundary.

## kNN

```{python, include=FALSE}

param_grid = {'n_neighbors': np.arange(1, 10)}

# instantiate the model
knn_model = KNeighborsClassifier()

knn_cv = GridSearchCV(knn_model, param_grid, cv=5)

# Fit the model to the training data
knn_cv.fit(x_train, y_train)

# Predict churn in the test set
knn_prediction_y = knn_cv.predict(x_test)

```

```{python, include=FALSE}

# Create dataframe with actual and predicted values of y
knn_prediction_y_series = pd.Series(knn_prediction_y, name = 'predicted_y')

y_test_actual = y_test.reset_index(drop = True).rename('actual_y')

df_knn_results = pd.concat([y_test_actual, knn_prediction_y_series], axis=1)

```

```{r}

# Create dataframe with results for the confusion matrix
df_knn_results <- py$df_knn_results %>%
  mutate(actual_y_text = case_when(
    actual_y == 1 ~ "Churn",
    actual_y == 0 ~ "No churn"),
    predicted_y_text = case_when(
      predicted_y == 1 ~ "Churn",
      predicted_y == 0 ~ "No churn"
    ))

```

```{r}

# Create confusion matrix
knn_confusion_matrix <- tbl_cross(
  df_knn_results,
  row = actual_y_text,
  col = predicted_y_text,
  margin=NULL,
  list(actual_y_text ~ "Actual label", predicted_y_text ~ "Predicted label")
) %>%
  modify_header(
    update = list(
      label ~ "")
  ) %>%
  modify_caption("**Confusion matrix of the kNN prediction**") %>%
  as_gt() %>%
  tab_style(
    style = list(
      cell_fill(color = "white")
    ),
    locations = cells_body(
      rows = everything())
  ) %>%
  tab_options(table.align='left',
              table.width = pct(100),
              column_labels.border.top.color = "white")

```

```{python, include=FALSE}

# Accuracy score
knn_accuracy_score = round(metrics.accuracy_score(y_test, knn_prediction_y), 2)

# Precision and recall scores for churn = 0
knn_precision_score_0 = round(metrics.precision_score(y_test, knn_prediction_y, pos_label = 0), 2)

knn_recall_score_0 = round(metrics.recall_score(y_test, knn_prediction_y, pos_label = 0), 2)

# Precision and recall scores for churn = 1
knn_precision_score_1 = round(metrics.precision_score(y_test, knn_prediction_y, pos_label = 1), 2)

knn_recall_score_1 = round(metrics.recall_score(y_test, knn_prediction_y, pos_label = 1), 2)
```

The second model I have used to predict churn is a kNN model. We can see the results of the prediction on the test set in the confusion matrix in table \@ref(tab:knn-confusion) below. The computed overall accuracy from the matrix is `r py$knn_accuracy_score`. This is a bit better than the logistic regression model (where the overall accuracy was `r py$logreg_accuracy_score`. Like the regression model, the kNN model is good at predicting "no churn", but struggles with predicting churn. We can, however, see a substantial improvement in the model's ability to predict churn compared to the logistic model. 

```{r knn-confusion}

knn_confusion_matrix

```

```{python, include=FALSE}

# Classification report where the output is a dictionary
report_knn_prediction = classification_report(y_test, knn_prediction_y, output_dict = True)

# Convert the dict to a dataframe
df_report_knn_prediction = pd.DataFrame(report_knn_prediction).transpose().reset_index().rename(columns={'index': 'bar'})

df_report_knn_prediction[['precision','recall', 'f1-score', 'support']] = df_report_knn_prediction[['precision','recall', 'f1-score', 'support']].apply(lambda x: pd.Series.round(x, 2))
```

```{r}

# Convert from pandas to R dataframe
df_report_knn_prediction <- py$df_report_knn_prediction %>%
  slice(c(1, 2, 5))

```

The classification report with precision, recall and specificity scores for the kNN model is presented in table \@ref(tab:knn-classification-report). Here we can see that the precision score for churn = 1 is `r py$knn_precision_score_1`. And the recall score is `r py$knn_recall_score_1`. This means that the kNN model is much better than the logistic regression model at predicting churn when the actual value is churn, and the model also makes fewer mistakes when it predicts churn. It's still not very impressive though. The model only predicts about 1/3 of actual churn as churn. 

```{r knn-classification-report}

# Publication ready classification report
gt(df_report_knn_prediction,
  caption = md("**Classification report of the KNN prediction**")) %>%
  tab_options(table.align='left',
              table.width = pct(100),
              column_labels.border.bottom.width= px(3),
              column_labels.border.bottom.color= "black",
              column_labels.border.top.color = "white") %>%
  tab_style(
    style = list(
      cell_text(weight = "bold")
      ),
    locations = cells_column_labels()) %>%
  tab_style(
    style = list(
      cell_fill(color = "white")
    ),
    locations = cells_body(
      rows = everything())
  ) %>% 
  tab_style(
    style = list(
       cell_text(weight = "bold")
    ),
    locations = cells_body(
      columns = 1)
  ) %>%
  cols_label(
    bar = ""
  )
  

```

## SVM

```{python, include=FALSE}

# Create the hyperparameter grid
parameters = {'C':[1, 10, 20, 30, 40], 'gamma':[0.01, 0.1, 1]}

# instantiate the model
svm_model = svm.SVC(probability=True)

# Instantiate the GridSearchCV object: svm_cv
svm_cv = GridSearchCV(svm_model, parameters)

# Fit the model to the training data
svm_cv.fit(x_train, y_train)

# Predict churn in the test set
svm_prediction_y = svm_cv.predict(x_test)

```

```{python, include=FALSE}

# Create dataframe with actual and predicted values of y
svm_prediction_y_series = pd.Series(svm_prediction_y, name = 'predicted_y')

y_test_actual = y_test.reset_index(drop = True).rename('actual_y')

df_svm_results = pd.concat([y_test_actual, svm_prediction_y_series], axis=1)

```

```{r}

# Create dataframe with results for the confusion matrix
df_svm_results <- py$df_svm_results %>%
  mutate(actual_y_text = case_when(
    actual_y == 1 ~ "Churn",
    actual_y == 0 ~ "No churn"),
    predicted_y_text = case_when(
      predicted_y == 1 ~ "Churn",
      predicted_y == 0 ~ "No churn"
    ))

```

```{r}

# Create confusion matrix
svm_confusion_matrix <- tbl_cross(
  df_svm_results,
  row = actual_y_text,
  col = predicted_y_text,
  margin=NULL,
  list(actual_y_text ~ "Actual label", predicted_y_text ~ "Predicted label")
) %>%
  modify_header(
    update = list(
      label ~ "")
  ) %>%
  modify_caption("**Confusion matrix of the SVM prediction**") %>%
  as_gt() %>%
  tab_style(
    style = list(
      cell_fill(color = "white")
    ),
    locations = cells_body(
      rows = everything())
  ) %>%
  tab_options(table.align='left',
              table.width = pct(100),
              column_labels.border.top.color = "white")

```


```{python, include=FALSE}

# Accuracy score
svm_accuracy_score = round(metrics.accuracy_score(y_test, svm_prediction_y), 2)

# Precision and recall scores for churn = 0
svm_precision_score_0 = round(metrics.precision_score(y_test, svm_prediction_y, pos_label = 0), 2)

svm_recall_score_0 = round(metrics.recall_score(y_test, svm_prediction_y, pos_label = 0), 2)

# Precision and recall scores for churn = 1
svm_precision_score_1 = round(metrics.precision_score(y_test, svm_prediction_y, pos_label = 1), 2)

svm_recall_score_1 = round(metrics.recall_score(y_test, svm_prediction_y, pos_label = 1), 2)
```

The final model I have used to predict churn is a SVM model. We can see the results of the prediction in the confusion matrix in table \@ref(tab:svm-confusion) below. The computed overall accuracy from the matrix is `r py$svm_accuracy_score`. Again we see an improvement compared to the two other models.

```{r svm-confusion}

svm_confusion_matrix

```

```{python, include=FALSE}

# Classification report where the output is a dictionary
report_svm_prediction = classification_report(y_test, svm_prediction_y, output_dict = True)

# Convert the dict to a dataframe
df_report_svm_prediction = pd.DataFrame(report_svm_prediction).transpose().reset_index().rename(columns={'index': 'bar'})

df_report_svm_prediction[['precision','recall', 'f1-score', 'support']] = df_report_svm_prediction[['precision','recall', 'f1-score', 'support']].apply(lambda x: pd.Series.round(x, 2))
```

```{r}

# Convert from pandas to R dataframe
df_report_svm_prediction <- py$df_report_svm_prediction %>%
  slice(c(1, 2, 5))

```

The classification report with precision, recall and specificity scores for the SVM model is presented in table \@ref(tab:svm-classification-report). Here we can see that the precision score for churn = 1 is `r py$knn_precision_score_1`. A non-trivial improvement compared to the kNN model. However, the biggest improvement between the two models is their respective recall scores. Or in other words the proportion of actual churn that are predicted as churn by the models. Here we see a jump from `r py$knn_recall_score_1` in the kNN model to `r py$svm_recall_score_1` in the SVM model. This means that the SVM model predicts almost 2/3 of actual churn as churn. Far from perfect, but much better than the two other models. Especially compared to the logistic regression model that had a recall score of `r py$logreg_precision_score_1`.

```{r svm-classification-report}

# Publication ready classification report
gt(df_report_svm_prediction,
  caption = md("**Classification report of the SVM prediction**")) %>%
  tab_options(table.align='left',
              table.width = pct(100),
              column_labels.border.bottom.width= px(3),
              column_labels.border.bottom.color= "black",
              column_labels.border.top.color = "white") %>%
  tab_style(
    style = list(
      cell_text(weight = "bold")
      ),
    locations = cells_column_labels()) %>%
  tab_style(
    style = list(
      cell_fill(color = "white")
    ),
    locations = cells_body(
      rows = everything())
  ) %>% 
  tab_style(
    style = list(
       cell_text(weight = "bold")
    ),
    locations = cells_body(
      columns = 1)
  ) %>%
  cols_label(
    bar = ""
  )
  

```

## ROC curves of the 3 models 

```{python}

# Compute predicted probabilities for all models for the ROC curves
y_logreg_prob = logreg_cv.predict_proba(x_test)[:,1]
y_knn_prob = knn_cv.predict_proba(x_test)[:,1]
y_svm_prob = svm_cv.predict_proba(x_test)[:,1]

# Generate ROC curve values: fpr, tpr, thresholds for models
fpr1, tpr1, thresh1 = roc_curve(y_test, y_logreg_prob, pos_label=1)
fpr2, tpr2, thresh2 = roc_curve(y_test, y_knn_prob, pos_label=1)
fpr3, tpr3, thresh3 = roc_curve(y_test, y_svm_prob, pos_label=1)

# AUC score for the SVM model
svm_auc_score = round(roc_auc_score(y_test, y_svm_prob), 2)

```


ROC curves provide an alternative to confusion matrices and classification reports in terms of evaluating the predictive performance of models. They do so by plotting the true positive rate (recall) versus the false positive rate (1 - specificity) at different probability thresholds (Irizarry 2019:511). In figure \@ref(fig:fig-roc) below the 3 ROC curves for all 3 models in this analysis are plotted. The ideal is the top-left corner, where the true positive rate is 1 and the false positive rate is 0. Thus the larger the area under the ROC curve, the better the model. This performance metric is called AUC ("Area Under the ROC Curve"). When the model perfectly predicts class membership the area under the ROC curve is 1. In the opposite scenario, where a model mispredicts all the values of the outcome variable, the area under the ROC curve will be 0. 

In the plot below we see — similar to the results presented previously — that the SVM model has the best performance. Its AUC score is `r py$svm_auc_score`. 

```{python fig-roc, warning=FALSE, message=FALSE, fig.cap="ROC curves for all 3 models"}

plt.plot(fpr1, tpr1, linestyle='--', label='Logistic Regression')
plt.plot(fpr2, tpr2, linestyle='--', label='K-Nearest Neighbors')
plt.plot(fpr3, tpr3, linestyle='--', label='Support Vector Machines')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for all 3 models')
plt.legend(loc='best')
plt.show()

#plt.clf()

```
## Concluding remarks
In this analysis I have explored how well different classification algorithms are able to predict customer churn in a data set with customer level information from a telecommunications company. The 3 included classification algorithms — logistic regression, k-nearest neighbors and Support Vector Machines — performed quite differently. The algorithm that performed the best was the Support Vector Machines algorithm.  

# Literature
Irizarry, Rafael A. (2019): Introduction to Data Science, CRC Press

