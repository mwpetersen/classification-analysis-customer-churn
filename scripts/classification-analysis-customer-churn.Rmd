---
title: "A classification analysis of customer churn"
subtitle: "Applied machine learning and big data, fall 2021"
author: "Mikkel Wittenburg Petersen"
date: "`r Sys.Date()`"
toc-title: Contents
output:
  pagedown::html_paged:
    toc: true
    fig_caption: true
    number_sections: true
    css:
      - "../styles/fonts.css"
      - default
      - "../styles/custom-style.css"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(reticulate)
library(gt)
library(gtsummary)
library(tidyverse)
```

```{python}
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.metrics import roc_curve, auc
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn import svm
import numpy as np

```

# Introduction

# Method

## The data set



# Analysis


```{python}
# Import the data
df_churn = pd.read_csv("../data/telecom-churn.csv")

```

```{python}
# #split dataset in features and target variable

x = df_churn.drop('Churn', axis=1)

y = df_churn['Churn']

```


```{python}
# split X and y into training and testing sets

x_train,x_test,y_train,y_test = train_test_split(x, y, test_size = 0.33,random_state = 0, stratify = y)

```


```{python}
# Standardize the predictor variables
scaler = StandardScaler()

x_train = scaler.fit_transform(x_train)

x_test = scaler.fit_transform(x_test)

```

## Logistic regression

```{python, include=FALSE}

# instantiate the model
logreg_model = LogisticRegression(solver='lbfgs', max_iter=1000)

# fit the model with data
logreg_model.fit(x_train,y_train)

# Predict churn in the test set
logreg_prediction_y = logreg_model.predict(x_test)

```

Model performance
```{python}

# Accuracy
churn_accuracy = metrics.accuracy_score(y_test, logreg_prediction_y)

churn_precision = metrics.precision_score(y_test, logreg_prediction_y)

churn_recall = metrics.recall_score(y_test, logreg_prediction_y)

```

The accuracy is: `r py$churn_accuracy`

The precision score (also called the positive predictive value (PPV)) is: `r py$churn_precision`. Its calculated by taking the number of true positives and dividing it with the number of true positives + the number of false positives. In the case of customer churn high precision means that the algorithm rarely predict churn when the customer has in fact stayed.

The recall score (also called sensitivity, hit rate or true positive rate) is: `r py$churn_recall`. Its calculated by taking the number of true positives and dividing it with the number of true positives + the number of false negatives. In the case of customer churn high recall means that the algorithm has predicted most churn correctly.


```{python}

# Create dataframe with actual and predicted values of y
logreg_prediction_y_series = pd.Series(logreg_prediction_y, name = 'predicted_y')

y_test_actual = y_test.reset_index(drop = True).rename('actual_y')

df_logreg_results = pd.concat([y_test_actual, logreg_prediction_y_series],axis=1)



```


```{r}

df_logreg_results <- py$df_logreg_results %>%
  mutate(actual_y_text = case_when(
    actual_y == 1 ~ "Churn",
    actual_y == 0 ~ "No churn"),
    predicted_y_text = case_when(
      predicted_y == 1 ~ "Churn",
      predicted_y == 0 ~ "No churn"
    ))

```

```{r}

logreg_confusion_matrix <- tbl_cross(
  df_logreg_results,
  row = actual_y_text,
  col = predicted_y_text,
  margin=NULL,
  list(actual_y_text ~ "Actual label", predicted_y_text ~ "Predicted label")
) %>%
  modify_header(
    update = list(
      label ~ "")
  ) %>%
  as_gt() %>%
  tab_style(
    style = list(
      cell_fill(color = "white")
    ),
    locations = cells_body(
      rows = everything())
  ) %>%
  tab_options(table.align='left',
              table.width = pct(100),
              column_labels.border.top.color = "white")

logreg_confusion_matrix  

```

```{python}

# Classification report where the output is a dictionary
report_logreg_prediction = classification_report(y_test, logreg_prediction_y, output_dict = True)

# Convert the dict to a dataframe
df_report_logreg_prediction = pd.DataFrame(report_logreg_prediction).transpose().reset_index().rename(columns={'index': 'bar'})

df_report_logreg_prediction[['precision','recall', 'f1-score', 'support']] = df_report_logreg_prediction[['precision','recall', 'f1-score', 'support']].apply(lambda x: pd.Series.round(x, 2))
```

```{r}

# Convert from pandas to R dataframe
df_report_logreg_prediction <- py$df_report_logreg_prediction

```

```{r}

# Publication ready classification report
gt(df_report_logreg_prediction) %>%
  tab_options(table.align='left',
              table.width = pct(100),
              column_labels.border.bottom.width= px(3),
              column_labels.border.bottom.color= "black",
              column_labels.border.top.color = "white") %>%
  tab_style(
    style = list(
      cell_text(weight = "bold")
      ),
    locations = cells_column_labels()) %>%
  tab_style(
    style = list(
      cell_fill(color = "white")
    ),
    locations = cells_body(
      rows = everything())
  ) %>% 
  tab_style(
    style = list(
       cell_text(weight = "bold")
    ),
    locations = cells_body(
      columns = 1)
  ) %>%
  cols_label(
    bar = ""
  )
  

```

ROC curve 
```{python}

# Compute predicted probabilities: y_pred_prob
y_pred_prob = logreg_model.predict_proba(x_test)[:,1]

# Generate ROC curve values: fpr, tpr, thresholds
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)

plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr, tpr)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.show()


```

## K-nearest neighbors

```{python}

# instantiate the model
knn_model = KNeighborsClassifier(n_neighbors=5, weights='distance')

# Fit the model to the training data
knn_model.fit(x_train, y_train)

# Predict churn in the test set
knn_prediction_y = knn_model.predict(x_test)

```

```{python}

# Create dataframe with actual and predicted values of y
knn_prediction_y_series = pd.Series(knn_prediction_y, name = 'predicted_y')

y_test_actual = y_test.reset_index(drop = True).rename('actual_y')

df_knn_results = pd.concat([y_test_actual, knn_prediction_y_series], axis=1)

```


```{r}

df_knn_results <- py$df_knn_results %>%
  mutate(actual_y_text = case_when(
    actual_y == 1 ~ "Churn",
    actual_y == 0 ~ "No churn"),
    predicted_y_text = case_when(
      predicted_y == 1 ~ "Churn",
      predicted_y == 0 ~ "No churn"
    ))

```

```{r}

knn_confusion_matrix <- tbl_cross(
  df_knn_results,
  row = actual_y_text,
  col = predicted_y_text,
  margin=NULL,
  list(actual_y_text ~ "Actual label", predicted_y_text ~ "Predicted label")
) %>%
  modify_header(
    update = list(
      label ~ "")
  ) %>%
  as_gt() %>%
  tab_style(
    style = list(
      cell_fill(color = "white")
    ),
    locations = cells_body(
      rows = everything())
  ) %>%
  tab_options(table.align='left',
              table.width = pct(100),
              column_labels.border.top.color = "white")

logreg_confusion_matrix  

```

```{python}

# Classification report where the output is a dictionary
report_knn_prediction = classification_report(y_test, knn_prediction_y, output_dict = True)

# Convert the dict to a dataframe
df_report_knn_prediction = pd.DataFrame(report_knn_prediction).transpose().reset_index().rename(columns={'index': 'bar'})

df_report_knn_prediction[['precision','recall', 'f1-score', 'support']] = df_report_knn_prediction[['precision','recall', 'f1-score', 'support']].apply(lambda x: pd.Series.round(x, 2))
```

```{r}

# Convert from pandas to R dataframe
df_report_knn_prediction <- py$df_report_knn_prediction

```

```{r}

# Publication ready classification report
gt(df_report_knn_prediction) %>%
  tab_options(table.align='left',
              table.width = pct(100),
              column_labels.border.bottom.width= px(3),
              column_labels.border.bottom.color= "black",
              column_labels.border.top.color = "white") %>%
  tab_style(
    style = list(
      cell_text(weight = "bold")
      ),
    locations = cells_column_labels()) %>%
  tab_style(
    style = list(
      cell_fill(color = "white")
    ),
    locations = cells_body(
      rows = everything())
  ) %>% 
  tab_style(
    style = list(
       cell_text(weight = "bold")
    ),
    locations = cells_body(
      columns = 1)
  ) %>%
  cols_label(
    bar = ""
  )
  

```

ROC curve 


## Support vector machines

```{python}

# instantiate the model
svm_model = svm.SVC()

# Fit the model to the training data
svm_model.fit(x_train, y_train)

# Predict churn in the test set
svm_prediction_y = svm_model.predict(x_test)

```


```{python}

# Create dataframe with actual and predicted values of y
svm_prediction_y_series = pd.Series(svm_prediction_y, name = 'predicted_y')

y_test_actual = y_test.reset_index(drop = True).rename('actual_y')

df_svm_results = pd.concat([y_test_actual, svm_prediction_y_series], axis=1)

```


```{r}

df_svm_results <- py$df_svm_results %>%
  mutate(actual_y_text = case_when(
    actual_y == 1 ~ "Churn",
    actual_y == 0 ~ "No churn"),
    predicted_y_text = case_when(
      predicted_y == 1 ~ "Churn",
      predicted_y == 0 ~ "No churn"
    ))

```

```{r}

svm_confusion_matrix <- tbl_cross(
  df_svm_results,
  row = actual_y_text,
  col = predicted_y_text,
  margin=NULL,
  list(actual_y_text ~ "Actual label", predicted_y_text ~ "Predicted label")
) %>%
  modify_header(
    update = list(
      label ~ "")
  ) %>%
  as_gt() %>%
  tab_style(
    style = list(
      cell_fill(color = "white")
    ),
    locations = cells_body(
      rows = everything())
  ) %>%
  tab_options(table.align='left',
              table.width = pct(100),
              column_labels.border.top.color = "white")

svm_confusion_matrix  

```

```{python}

# Classification report where the output is a dictionary
report_svm_prediction = classification_report(y_test, svm_prediction_y, output_dict = True)

# Convert the dict to a dataframe
df_report_svm_prediction = pd.DataFrame(report_svm_prediction).transpose().reset_index().rename(columns={'index': 'bar'})

df_report_svm_prediction[['precision','recall', 'f1-score', 'support']] = df_report_svm_prediction[['precision','recall', 'f1-score', 'support']].apply(lambda x: pd.Series.round(x, 2))
```

```{r}

# Convert from pandas to R dataframe
df_report_svm_prediction <- py$df_report_svm_prediction

```

```{r}

# Publication ready classification report
gt(df_report_svm_prediction) %>%
  tab_options(table.align='left',
              table.width = pct(100),
              column_labels.border.bottom.width= px(3),
              column_labels.border.bottom.color= "black",
              column_labels.border.top.color = "white") %>%
  tab_style(
    style = list(
      cell_text(weight = "bold")
      ),
    locations = cells_column_labels()) %>%
  tab_style(
    style = list(
      cell_fill(color = "white")
    ),
    locations = cells_body(
      rows = everything())
  ) %>% 
  tab_style(
    style = list(
       cell_text(weight = "bold")
    ),
    locations = cells_body(
      columns = 1)
  ) %>%
  cols_label(
    bar = ""
  )
  

```

ROC curve 

# Literature


